from pydantic import BaseModel, Field
from typing import List, Optional

class BaseFeedbackOutput(BaseModel):
    """Base model for consistent error handling or metadata if needed."""
    raw_llm_output: Optional[str] = Field(None, description="The raw text output from the LLM before parsing.")
    parsing_error: Optional[str] = Field(None, description="Error message if parsing the LLM output failed.")

class TextFeedbackOutput(BaseFeedbackOutput):
    """For modules that are expected to return a single block of text feedback."""
    feedback_text: str = Field(description="The main feedback text generated by the LLM.")

class ScoreAndJustificationOutput(BaseFeedbackOutput):
    """For modules that provide a score and a textual justification."""
    score: float = Field(description="The calculated score.", ge=0)
    justification: str = Field(description="The justification for the given score.")

class SuggestionsOutput(BaseFeedbackOutput):
    """For modules that provide a list of suggestions."""
    suggestions: List[str] = Field(description="A list of suggestions.")

class DetailedFeedbackOutput(BaseFeedbackOutput):
    """A more complex example combining several elements."""
    overall_comment: str = Field(description="An overall comment on the section.")
    strengths: List[str] = Field(default_factory=list, description="Identified strengths.")
    areas_for_improvement: List[str] = Field(default_factory=list, description="Identified areas for improvement.")
    specific_line_references: Optional[List[str]] = Field(None, description="References to specific lines of code or text, if applicable.")

# A dictionary to easily map model names (from config) to model classes
OUTPUT_MODEL_REGISTRY = {
    "TextFeedback": TextFeedbackOutput,
    "ScoreAndJustification": ScoreAndJustificationOutput,
    "Suggestions": SuggestionsOutput,
    "DetailedFeedback": DetailedFeedbackOutput,
} 